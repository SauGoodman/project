import json
import logging
import awswrangler as wr
import requests
import pandas as pd
from botocore.exceptions import ClientError

# ログ設定の初期化
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    try:
        # ステップ1: S3からCSVファイルを読み込む
        bucket_name = "mastergpoc"
        input_file_key = "temp/01_product_compare.csv"  # 入力ファイルのS3パス
        post_file_key = "results/01_product_apipost.csv"  # POST用結果ファイルのS3パス
        put_file_key = "results/01_product_apiput.csv"  # PUT用結果ファイルのS3パス

        logger.info(f"S3からファイルを読み込み中: s3://{bucket_name}/{input_file_key}")

        # CSVファイルの読み込み時に特定の列を文字列型に設定
        try:
            data = wr.s3.read_csv(
                f's3://{bucket_name}/{input_file_key}',
                dtype={"buturyu_cd": str}  # 'buturyu_cd'列を文字列として読み込む
            )
        except ClientError as e:
            logger.error(f"S3ファイルの読み込みに失敗しました: {str(e)}")
            return {"statusCode": 500, "body": f"S3ファイルの読み込みに失敗しました: {str(e)}"}

        # データ型の確認ログ出力
        logger.info(f"データ型確認: {data.dtypes}")

        # 先頭ゼロが消えないように明示的に文字列変換を強制
        data["buturyu_cd"] = data["buturyu_cd"].astype(str).str.zfill(5)

        logger.info(f"ゼロ埋め後のデータサンプル: {data[['buturyu_cd']].head()}")

        # ステップ2: APIトークンとヘッダーの設定
        token = "Bearer your_api_token_here"  # APIトークンは必要に応じて変更
        headers = {
            "User-Agent": "test-agent",
            "Authorization": token,
            "ClientId": "75"
        }

        # ステップ3: APIリクエストの送信（100レコードごとの分割処理）
        chunk_size = 100
        post_df_list = []
        put_df_list = []

        for i in range(0, len(data), chunk_size):
            chunk = data.iloc[i:i + chunk_size]
            buturyu_cd_list = chunk['buturyu_cd'].tolist()
            joined_buturyu_cd = "&".join(map(str, buturyu_cd_list))
            api_url = f"https://api.plapi-product.com/plapi/api/v1/com_product_id/catalogs?{joined_buturyu_cd}"

            logger.info(f"APIリクエスト送信中: {api_url}")
            response = requests.get(api_url, headers=headers)

            if response.status_code not in [200, 400]:
                logger.error(f"APIリクエスト失敗: {response.status_code} - {response.text}")
                continue

            # レスポンス解析
            try:
                api_data = response.json()
            except json.JSONDecodeError:
                logger.error(f"APIレスポンスのデコードに失敗しました: {response.text}")
                continue

            errors = api_data.get("errors", [])
            templates = api_data.get("templates", [])

            error_ids = {str(error["com_product_id"]) for error in errors}
            template_dict = {str(template["com_product_id"]): template["product_id"] for template in templates}

            # POSTとPUTデータに分割
            post_df = chunk[chunk["buturyu_cd"].isin(error_ids)]
            put_df = chunk[chunk["buturyu_cd"].isin(template_dict.keys())]

            # PUTデータにproduct_idを追加
            put_df["product_id"] = put_df["buturyu_cd"].map(template_dict)
            put_df = put_df[["product_id"] + list(data.columns)]

            post_df_list.append(post_df)
            put_df_list.append(put_df)

        # 結果を結合
        post_df = pd.concat(post_df_list, ignore_index=True) if post_df_list else pd.DataFrame()
        put_df = pd.concat(put_df_list, ignore_index=True) if put_df_list else pd.DataFrame()

        # ステップ4: S3に保存
        if not post_df.empty:
            wr.s3.to_csv(post_df, f's3://{bucket_name}/{post_file_key}', index=False)
            logger.info(f"POST結果を保存しました: s3://{bucket_name}/{post_file_key}")

        if not put_df.empty:
            wr.s3.to_csv(put_df, f's3://{bucket_name}/{put_file_key}', index=False)
            logger.info(f"PUT結果を保存しました: s3://{bucket_name}/{put_file_key}")

        logger.info("処理が正常に完了しました。")
        return {"statusCode": 200, "body": json.dumps("処理が正常に完了しました。")}

    except Exception as e:
        logger.error(f"エラーが発生しました: {str(e)}")
        return {"statusCode": 500, "body": f"エラー: {str(e)}"}