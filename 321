import json
import logging
import awswrangler as wr
import requests
import pandas as pd
import boto3
import zipfile
import io
from botocore.exceptions import ClientError

# ログ設定
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# DynamoDB 設定
dynamodb = boto3.resource('dynamodb')
table_name = "RequestCount"
request_count_table = dynamodb.Table(table_name)

# S3 クライアント
s3 = boto3.client('s3')

# APIトークンとヘッダの設定
token = "Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9..."
headers = {
    "User-Agent": "test-agent",
    "Authorization": token,
    "ClientId": "75"
}

# DynamoDB関連関数
def get_last_processed_count(function_type):
    try:
        logger.info(f"DynamoDBから件数取得: FunctionType={function_type}")
        response = request_count_table.get_item(Key={'FunctionType': function_type})
        return int(response['Item']['RecCount']) if 'Item' in response else 0
    except Exception as e:
        logger.error(f"DynamoDB取得エラー: {e}")
        return 0

def update_request_count(function_type, count):
    try:
        logger.info(f"DynamoDBに件数更新: FunctionType={function_type}, RecCount={count}")
        request_count_table.update_item(
            Key={'FunctionType': function_type},
            UpdateExpression="SET RecCount = :count",
            ExpressionAttributeValues={':count': count}
        )
    except ClientError as e:
        logger.error(f"DynamoDB更新エラー: {e}")

# ZIP ファイルの処理
def extract_csv_from_zip(bucket_name, file_key):
    logger.info(f"ZIPファイルを解凍: {file_key}")
    response = s3.get_object(Bucket=bucket_name, Key=file_key)
    with zipfile.ZipFile(io.BytesIO(response['Body'].read())) as z:
        # 1つのCSVファイルのみを処理
        for file in z.namelist():
            if file.endswith(".csv"):
                logger.info(f"CSVファイルを抽出: {file}")
                with z.open(file) as f:
                    return pd.read_csv(f, dtype=str)
    raise ValueError("ZIPファイル内にCSVファイルが見つかりません。")

# Lambdaハンドラー関数
def lambda_handler(event, context):
    try:
        bucket_name = "mastergpoc"
        input_file_key = event.get("input_file_key", "")  # イベントからファイルキーを取得
        logger.info(f"S3ファイル読み込み: s3://{bucket_name}/{input_file_key}")

        # ファイルの種類を判定して読み込み
        if input_file_key.endswith(".zip"):
            df = extract_csv_from_zip(bucket_name, input_file_key)
        elif input_file_key.endswith(".csv"):
            df = wr.s3.read_csv(f's3://{bucket_name}/{input_file_key}', dtype=str, keep_default_na=False)
        else:
            logger.error(f"サポートされていないファイル形式: {input_file_key}")
            return {"statusCode": 400, "body": f"エラー: サポートされていないファイル形式です: {input_file_key}"}

        logger.info(f"読み込んだCSVの列: {df.columns}")

        # 処理の種類をファイル名から判定
        if "apipost" in input_file_key:
            function_type = "POST"
            last_count = get_last_processed_count("POST")
            filtered_df = df.iloc[last_count:]
            logger.info(f"POSTデータ: {len(filtered_df)} 件")
            update_request_count("POST", last_count + len(filtered_df))
        elif "apiput" in input_file_key:
            function_type = "PUT"
            last_count = get_last_processed_count("PUT")
            filtered_df = df.iloc[last_count:]
            logger.info(f"PUTデータ: {len(filtered_df)} 件")
            update_request_count("PUT", last_count + len(filtered_df))
        else:
            logger.error("無効なファイル名です。POSTまたはPUTの処理が必要です。")
            return {"statusCode": 400, "body": "無効なファイル名です。POSTまたはPUTが含まれていません。"}

        return {"statusCode": 200, "body": "処理が正常に完了しました"}

    except Exception as e:
        logger.error(f"エラーが発生しました: {e}")
        return {"statusCode": 500, "body": f"エラー: {e}"}